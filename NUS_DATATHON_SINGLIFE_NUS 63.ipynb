{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas \n",
    "%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imbalanced-learn\n",
    "%pip install seaborn\n",
    "%pip install tensorflow\n",
    "%pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['f_purchase_lh'] = data['f_purchase_lh'].fillna(0)\n",
    "y = data['f_purchase_lh']\n",
    "data = data.drop(['f_purchase_lh'],axis = 1)\n",
    "data = data.drop(['clntnum'], axis = 1)\n",
    "data = data.drop(['min_occ_date'], axis = 1)\n",
    "data = data.drop(['cltdob_fix'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting data columns with more than 50% missing data\n",
    "thresh = 50\n",
    "percentage = [col for col in data if data[col].isna().sum()/data.shape[0]*100>thresh]\n",
    "data = data.drop(percentage, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_data = [col for col in data.columns if data[col].dtype not in ['float64', 'int64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cate_data:\n",
    "    mode_value = data[col].mode().iloc[0]  # Use iloc[0] to get the first mode in case of multiple modes\n",
    "    data[col] = data[col].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually filtered list of numerical values classified as categorical data\n",
    "\n",
    "decimal_columns = ['hh_20', 'pop_20', 'hh_size', 'n_months_last_bought_products', 'flg_latest_being_lapse', 'flg_latest_being_cancel', 'tot_inforce_pols',\n",
    "                    'ape_gi_42e115', 'ape_ltc_1280bf', 'ape_grp_6fc3e6', 'ape_grp_de05ae', 'ape_inv_dcd836', 'ape_grp_945b5a',\n",
    "                  'ape_grp_6a5788', 'ape_ltc_43b9d5', 'ape_grp_9cdedf', 'ape_lh_d0adeb', 'ape_grp_1581d7', 'ape_grp_22decf',\n",
    "                  'ape_lh_507c37', 'ape_lh_839f8a', 'ape_inv_e9f316', 'ape_gi_a10d1b', 'ape_gi_29d435', 'ape_grp_caa6ff', \n",
    "                  'ape_grp_fd3bfb', 'ape_lh_e22a6a', 'ape_grp_70e1dd', 'ape_grp_e04c3a', 'ape_grp_fe5fb8', 'ape_gi_856320', \n",
    "                   'ape_grp_94baec', 'ape_gi_058815', 'ape_grp_e91421', 'ape_lh_f852af', 'ape_lh_947b15', 'sumins_gi_42e115', \n",
    "                   'sumins_ltc_1280bf', 'sumins_grp_6fc3e6', 'sumins_grp_de05ae', 'sumins_inv_dcd836', 'sumins_grp_945b5a', 'sumins_grp_6a5788', \n",
    "                  'sumins_ltc_43b9d5', 'sumins_grp_9cdedf', 'sumins_lh_d0adeb', 'sumins_grp_1581d7', 'sumins_grp_22decf', 'sumins_lh_507c37', \n",
    "                  'sumins_inv_e9f316', 'sumins_gi_a10d1b', 'sumins_gi_29d435', 'sumins_grp_caa6ff', 'sumins_grp_fd3bfb', 'sumins_lh_e22a6a', \n",
    "                  'sumins_grp_70e1dd', 'sumins_grp_e04c3a', 'sumins_grp_fe5fb8', 'sumins_gi_856320', 'sumins_grp_94baec', 'sumins_gi_058815',\n",
    "                  'sumins_grp_e91421', 'sumins_lh_f852af', 'sumins_lh_947b15', 'sumins_32c74c', 'prempaid_gi_42e115', 'prempaid_ltc_1280bf', \n",
    "                  'prempaid_grp_6fc3e6', 'prempaid_grp_de05ae', 'prempaid_inv_dcd836', 'prempaid_grp_945b5a', 'prempaid_grp_6a5788', \n",
    "                  'prempaid_ltc_43b9d5', 'prempaid_grp_9cdedf', 'prempaid_lh_d0adeb', 'prempaid_grp_1581d7', 'prempaid_lh_507c37', 'prempaid_lh_839f8a', \n",
    "                  'prempaid_inv_e9f316', 'prempaid_gi_a10d1b', 'prempaid_gi_29d435', 'prempaid_grp_caa6ff', 'prempaid_grp_fd3bfb', 'prempaid_lh_e22a6a', \n",
    "                  'prempaid_grp_70e1dd', 'prempaid_grp_e04c3a', 'prempaid_grp_fe5fb8', 'prempaid_gi_856320', 'prempaid_grp_94baec', \n",
    "                  'prempaid_gi_058815', 'prempaid_grp_e91421', 'prempaid_lh_f852af', 'prempaid_lh_947b15', 'prempaid_32c74c', \n",
    "                  'ape_839f8a', 'ape_e22a6a', 'ape_d0adeb', 'ape_c4bda5', 'ape_ltc', 'ape_507c37', 'ape_gi', 'f_hold_839f8a', \n",
    "                  'f_hold_e22a6a', 'f_hold_d0adeb', 'f_hold_c4bda5', 'f_hold_ltc', 'f_hold_507c37', 'f_hold_gi', 'sumins_839f8a',\n",
    "                  'sumins_e22a6a', 'sumins_d0adeb', 'sumins_c4bda5', 'sumins_ltc', 'sumins_507c37', 'sumins_gi', 'prempaid_839f8a', 'prempaid_e22a6a', \n",
    "                  'prempaid_d0adeb', 'prempaid_c4bda5', 'prempaid_ltc', 'prempaid_507c37', 'prempaid_gi', 'n_months_last_bought_839f8a',\n",
    "                  'n_months_last_bought_e22a6a', 'n_months_last_bought_d0adeb', 'n_months_last_bought_c4bda5', 'n_months_last_bought_ltc', \n",
    "                   'n_months_last_bought_507c37', 'n_months_last_bought_gi', 'f_ever_bought_ltc_1280bf', 'f_ever_bought_grp_6fc3e6', 'f_ever_bought_grp_de05ae', \n",
    "                   'f_ever_bought_inv_dcd836', 'f_ever_bought_grp_945b5a', 'f_ever_bought_grp_6a5788', 'f_ever_bought_ltc_43b9d5', 'f_ever_bought_grp_9cdedf',\n",
    "                   'f_ever_bought_lh_d0adeb', 'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf', 'f_ever_bought_lh_507c37', 'f_ever_bought_lh_839f8a',\n",
    "                   'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff', 'f_ever_bought_grp_fd3bfb', 'f_ever_bought_lh_e22a6a', 'f_ever_bought_grp_70e1dd',\n",
    "                   'f_ever_bought_grp_e04c3a', 'f_ever_bought_grp_fe5fb8', 'f_ever_bought_grp_94baec', 'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af',\n",
    "                   'f_ever_bought_lh_947b15', 'f_ever_bought_32c74c', 'n_months_last_bought_ltc_1280bf', 'n_months_last_bought_grp_6fc3e6', 'n_months_last_bought_grp_de05ae', \n",
    "                  'n_months_last_bought_inv_dcd836', 'n_months_last_bought_grp_945b5a', 'n_months_last_bought_grp_6a5788', 'n_months_last_bought_ltc_43b9d5', \n",
    "                  'n_months_last_bought_grp_9cdedf', 'n_months_last_bought_lh_d0adeb', 'n_months_last_bought_grp_1581d7', 'n_months_last_bought_grp_22decf', \n",
    "                  'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_839f8a', 'n_months_last_bought_inv_e9f316', \n",
    "                  'n_months_last_bought_grp_caa6ff', 'n_months_last_bought_grp_fd3bfb', 'n_months_last_bought_lh_e22a6a', 'n_months_last_bought_grp_70e1dd', \n",
    "                  'n_months_last_bought_grp_e04c3a', 'n_months_last_bought_grp_fe5fb8', 'n_months_last_bought_grp_94baec', 'n_months_last_bought_grp_e91421', \n",
    "                  'n_months_last_bought_lh_f852af', 'n_months_last_bought_lh_947b15', 'n_months_last_bought_32c74c', 'f_elx', 'f_mindef_mha', 'f_retail', \n",
    "                   'ape_32c74c', 'prempaid_grp_22decf', 'f_ever_bought_839f8a', 'f_ever_bought_e22a6a', 'f_ever_bought_d0adeb', 'f_ever_bought_c4bda5',\n",
    "                   'f_ever_bought_ltc', 'f_ever_bought_507c37', 'f_ever_bought_gi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting data with Decimal('') to float and other string representations of numbers into floats\n",
    "for col in decimal_columns:\n",
    "    data[col] = list(map(float, data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding for nominal data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "processed_columns = ['race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'cltsex_fix',\n",
    "                     'flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', \n",
    "                     'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', \n",
    "                    'flg_with_preauthorisation', 'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
    "                    'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email', 'is_housewife_retiree',\n",
    "                    'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy', 'hh_size_est', 'annual_income_est']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=False, drop='first'), processed_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through non-categorical columns\n",
    ")\n",
    "data_encoded = preprocessor.fit_transform(data)\n",
    "data = pd.DataFrame(data_encoded, columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = [col for col in data.columns if data[col].dtype in ['float64', 'int64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_data:\n",
    "    median_value = data[col].median()  # Use iloc[0] to get the first mode in case of multiple modes\n",
    "    data[col] = data[col].fillna(median_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection to reduce number of features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X = data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "rf_classifier_0 = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "\n",
    "rf_classifier_0.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = rf_classifier_0.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.001\n",
    "selected_features = feature_importance_df.loc[feature_importance_df['Importance'] > threshold, 'Feature']\n",
    "\n",
    "X_selected = X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing imbalance of the classes\n",
    "import matplotlib.pyplot as plt\n",
    "positive_ratio = (y == 1).sum() / len(data)\n",
    "negative_ratio = (y== 0).sum() / len(data)\n",
    "labels = ['Positive', 'Negative']\n",
    "ratios = [positive_ratio, negative_ratio]\n",
    "\n",
    "plt.bar(labels, ratios, color=['green', 'red'])\n",
    "plt.title('Positive/Negative Ratio')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Ratio')\n",
    "plt.show()\n",
    "print(positive_ratio)\n",
    "print(negative_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns with no variance\n",
    "X = X_selected\n",
    "zero_std_columns = X.columns[X.std() == 0]\n",
    "X = X.drop(columns=zero_std_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into train, test and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling the majority class to 90:10\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "under_sample = RandomUnderSampler(sampling_strategy = 1/9, random_state = 10)\n",
    "X_train_sampled, y_train_sampled = under_sample.fit_resample(X_train, y_train)\n",
    "count_1 = (y_train_sampled == 1).sum()/len(y_train_sampled)\n",
    "count_0 = (y_train_sampled == 0).sum()/len(y_train_sampled)\n",
    "labels = ['Positive', 'Negative']\n",
    "ratios = [count_1, count_0]\n",
    "\n",
    "plt.bar(labels, ratios, color=['green', 'red'])\n",
    "plt.title('Positive/Negative Ratio after undersampling')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Ratio')\n",
    "plt.show()\n",
    "print(count_1)\n",
    "print(count_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE the minority class\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy = 1.1/8.9, random_state = 10)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_sampled, y_train_sampled)\n",
    "count_1_smote = (y_train_smote == 1).sum()/len(y_train_smote)\n",
    "count_0_smote = (y_train_smote == 0).sum()/len(y_train_smote)\n",
    "labels = ['Positive', 'Negative']\n",
    "ratios = [count_1_smote, count_0_smote]\n",
    "\n",
    "plt.bar(labels, ratios, color=['green', 'red'])\n",
    "plt.title('Positive/Negative Ratio after SMOTE')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Ratio')\n",
    "plt.show()\n",
    "print(count_1_smote)\n",
    "print(count_0_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "param_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]}\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced',random_state=10)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_rf, cv=5, scoring='f1')\n",
    "grid_search_rf.fit(X_train_smote, y_train_smote)\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_estimator_rf = grid_search_rf.best_estimator_\n",
    "print(\"Best Parameters:\", best_params_rf)\n",
    "y_pred_rf = best_estimator_rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "f1_rf = f1_score(y_test,y_pred_rf)\n",
    "print(\"f1_score:\", f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    filepath = \"./data/catB_train.parquet\" \n",
    "    data = pd.read_parquet(filepath)\n",
    "    #Taking in the data and removing unimportant columns and target column, target column assigned to y\n",
    "    data['f_purchase_lh'] = data['f_purchase_lh'].fillna(0)\n",
    "    y = data['f_purchase_lh']\n",
    "    data = data.drop(['f_purchase_lh'],axis = 1)\n",
    "    data = data.drop(['clntnum'], axis = 1)\n",
    "    data = data.drop(['min_occ_date'], axis = 1)\n",
    "    data = data.drop(['cltdob_fix'], axis = 1)\n",
    "\n",
    "    #Deleting data columns with more than 50% missing data\n",
    "    thresh = 50\n",
    "    percentage = [col for col in data if data[col].isna().sum()/data.shape[0]*100>thresh]\n",
    "    data = data.drop(percentage, axis=1)\n",
    "\n",
    "    #Imputing for categorical data\n",
    "    cate_data = [col for col in data.columns if data[col].dtype not in ['float64', 'int64']]\n",
    "    for col in cate_data:\n",
    "        mode_value = data[col].mode().iloc[0]  # Use iloc[0] to get the first mode in case of multiple modes\n",
    "        data[col] = data[col].fillna(mode_value)\n",
    "\n",
    "    # manually filtered list of numerical values classified as categorical data\n",
    "    decimal_columns = ['hh_20', 'pop_20', 'hh_size', 'n_months_last_bought_products', 'flg_latest_being_lapse', 'flg_latest_being_cancel', 'tot_inforce_pols',\n",
    "                    'ape_gi_42e115', 'ape_ltc_1280bf', 'ape_grp_6fc3e6', 'ape_grp_de05ae', 'ape_inv_dcd836', 'ape_grp_945b5a',\n",
    "                  'ape_grp_6a5788', 'ape_ltc_43b9d5', 'ape_grp_9cdedf', 'ape_lh_d0adeb', 'ape_grp_1581d7', 'ape_grp_22decf',\n",
    "                  'ape_lh_507c37', 'ape_lh_839f8a', 'ape_inv_e9f316', 'ape_gi_a10d1b', 'ape_gi_29d435', 'ape_grp_caa6ff', \n",
    "                  'ape_grp_fd3bfb', 'ape_lh_e22a6a', 'ape_grp_70e1dd', 'ape_grp_e04c3a', 'ape_grp_fe5fb8', 'ape_gi_856320', \n",
    "                   'ape_grp_94baec', 'ape_gi_058815', 'ape_grp_e91421', 'ape_lh_f852af', 'ape_lh_947b15', 'sumins_gi_42e115', \n",
    "                   'sumins_ltc_1280bf', 'sumins_grp_6fc3e6', 'sumins_grp_de05ae', 'sumins_inv_dcd836', 'sumins_grp_945b5a', 'sumins_grp_6a5788', \n",
    "                  'sumins_ltc_43b9d5', 'sumins_grp_9cdedf', 'sumins_lh_d0adeb', 'sumins_grp_1581d7', 'sumins_grp_22decf', 'sumins_lh_507c37', \n",
    "                  'sumins_inv_e9f316', 'sumins_gi_a10d1b', 'sumins_gi_29d435', 'sumins_grp_caa6ff', 'sumins_grp_fd3bfb', 'sumins_lh_e22a6a', \n",
    "                  'sumins_grp_70e1dd', 'sumins_grp_e04c3a', 'sumins_grp_fe5fb8', 'sumins_gi_856320', 'sumins_grp_94baec', 'sumins_gi_058815',\n",
    "                  'sumins_grp_e91421', 'sumins_lh_f852af', 'sumins_lh_947b15', 'sumins_32c74c', 'prempaid_gi_42e115', 'prempaid_ltc_1280bf', \n",
    "                  'prempaid_grp_6fc3e6', 'prempaid_grp_de05ae', 'prempaid_inv_dcd836', 'prempaid_grp_945b5a', 'prempaid_grp_6a5788', \n",
    "                  'prempaid_ltc_43b9d5', 'prempaid_grp_9cdedf', 'prempaid_lh_d0adeb', 'prempaid_grp_1581d7', 'prempaid_lh_507c37', 'prempaid_lh_839f8a', \n",
    "                  'prempaid_inv_e9f316', 'prempaid_gi_a10d1b', 'prempaid_gi_29d435', 'prempaid_grp_caa6ff', 'prempaid_grp_fd3bfb', 'prempaid_lh_e22a6a', \n",
    "                  'prempaid_grp_70e1dd', 'prempaid_grp_e04c3a', 'prempaid_grp_fe5fb8', 'prempaid_gi_856320', 'prempaid_grp_94baec', \n",
    "                  'prempaid_gi_058815', 'prempaid_grp_e91421', 'prempaid_lh_f852af', 'prempaid_lh_947b15', 'prempaid_32c74c', \n",
    "                  'ape_839f8a', 'ape_e22a6a', 'ape_d0adeb', 'ape_c4bda5', 'ape_ltc', 'ape_507c37', 'ape_gi', 'f_hold_839f8a', \n",
    "                  'f_hold_e22a6a', 'f_hold_d0adeb', 'f_hold_c4bda5', 'f_hold_ltc', 'f_hold_507c37', 'f_hold_gi', 'sumins_839f8a',\n",
    "                  'sumins_e22a6a', 'sumins_d0adeb', 'sumins_c4bda5', 'sumins_ltc', 'sumins_507c37', 'sumins_gi', 'prempaid_839f8a', 'prempaid_e22a6a', \n",
    "                  'prempaid_d0adeb', 'prempaid_c4bda5', 'prempaid_ltc', 'prempaid_507c37', 'prempaid_gi', 'n_months_last_bought_839f8a',\n",
    "                  'n_months_last_bought_e22a6a', 'n_months_last_bought_d0adeb', 'n_months_last_bought_c4bda5', 'n_months_last_bought_ltc', \n",
    "                   'n_months_last_bought_507c37', 'n_months_last_bought_gi', 'f_ever_bought_ltc_1280bf', 'f_ever_bought_grp_6fc3e6', 'f_ever_bought_grp_de05ae', \n",
    "                   'f_ever_bought_inv_dcd836', 'f_ever_bought_grp_945b5a', 'f_ever_bought_grp_6a5788', 'f_ever_bought_ltc_43b9d5', 'f_ever_bought_grp_9cdedf',\n",
    "                   'f_ever_bought_lh_d0adeb', 'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf', 'f_ever_bought_lh_507c37', 'f_ever_bought_lh_839f8a',\n",
    "                   'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff', 'f_ever_bought_grp_fd3bfb', 'f_ever_bought_lh_e22a6a', 'f_ever_bought_grp_70e1dd',\n",
    "                   'f_ever_bought_grp_e04c3a', 'f_ever_bought_grp_fe5fb8', 'f_ever_bought_grp_94baec', 'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af',\n",
    "                   'f_ever_bought_lh_947b15', 'f_ever_bought_32c74c', 'n_months_last_bought_ltc_1280bf', 'n_months_last_bought_grp_6fc3e6', 'n_months_last_bought_grp_de05ae', \n",
    "                  'n_months_last_bought_inv_dcd836', 'n_months_last_bought_grp_945b5a', 'n_months_last_bought_grp_6a5788', 'n_months_last_bought_ltc_43b9d5', \n",
    "                  'n_months_last_bought_grp_9cdedf', 'n_months_last_bought_lh_d0adeb', 'n_months_last_bought_grp_1581d7', 'n_months_last_bought_grp_22decf', \n",
    "                  'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_839f8a', 'n_months_last_bought_inv_e9f316', \n",
    "                  'n_months_last_bought_grp_caa6ff', 'n_months_last_bought_grp_fd3bfb', 'n_months_last_bought_lh_e22a6a', 'n_months_last_bought_grp_70e1dd', \n",
    "                  'n_months_last_bought_grp_e04c3a', 'n_months_last_bought_grp_fe5fb8', 'n_months_last_bought_grp_94baec', 'n_months_last_bought_grp_e91421', \n",
    "                  'n_months_last_bought_lh_f852af', 'n_months_last_bought_lh_947b15', 'n_months_last_bought_32c74c', 'f_elx', 'f_mindef_mha', 'f_retail', \n",
    "                   'ape_32c74c', 'prempaid_grp_22decf', 'f_ever_bought_839f8a', 'f_ever_bought_e22a6a', 'f_ever_bought_d0adeb', 'f_ever_bought_c4bda5',\n",
    "                   'f_ever_bought_ltc', 'f_ever_bought_507c37', 'f_ever_bought_gi']\n",
    "\n",
    "    # Formatting data with Decimal('') to float and other string representations of numbers into floats\n",
    "    for col in decimal_columns:\n",
    "        data[col] = list(map(float, data[col]))\n",
    "\n",
    "    #One Hot Encoding for nominal data\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    processed_columns = ['race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'cltsex_fix',\n",
    "                     'flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', \n",
    "                     'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', \n",
    "                    'flg_with_preauthorisation', 'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
    "                    'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email', 'is_housewife_retiree',\n",
    "                    'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy', 'hh_size_est', 'annual_income_est']\n",
    "    preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(sparse_output=False, drop='first'), processed_columns)],remainder='passthrough')\n",
    "    data_encoded = preprocessor.fit_transform(data)\n",
    "    data = pd.DataFrame(data_encoded, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "    #Imputing for numerical data\n",
    "    num_data = [col for col in data.columns if data[col].dtype in ['float64', 'int64']]\n",
    "    for col in num_data:\n",
    "        median_value = data[col].median()\n",
    "        data[col] = data[col].fillna(median_value)\n",
    "\n",
    "    #Feature selection to reduce number of features\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    X = data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    rf_classifier_0 = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "    rf_classifier_0.fit(X_train, y_train)\n",
    "    feature_importances = rf_classifier_0.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "    threshold = 0.001\n",
    "    selected_features = feature_importance_df.loc[feature_importance_df['Importance'] > threshold, 'Feature']\n",
    "    X_selected = X[selected_features]\n",
    "\n",
    "    #Dropping columns with no variance\n",
    "    X = X_selected\n",
    "    zero_std_columns = X.columns[X.std() == 0]\n",
    "    X = X.drop(columns=zero_std_columns)\n",
    "\n",
    "    #Standardising the data\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "\n",
    "    #Splitting the dataset into train, test and validation sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.25, random_state=10)\n",
    "\n",
    "    #Undersampling the majority class\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    under_sample = RandomUnderSampler(sampling_strategy = 1/9, random_state = 10)\n",
    "    X_train_sampled, y_train_sampled = under_sample.fit_resample(X_train, y_train)\n",
    "\n",
    "    #SMOTE the minority class\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(sampling_strategy = 1.1/8.9, random_state = 10)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_sampled, y_train_sampled)\n",
    "\n",
    "    #Random Forest Classifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_rf = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]}\n",
    "    rf_classifier = RandomForestClassifier(class_weight='balanced',random_state=10)\n",
    "    grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_rf, cv=5, scoring='f1')\n",
    "    grid_search_rf.fit(X_train_smote, y_train_smote)\n",
    "    best_params_rf = grid_search_rf.best_params_\n",
    "    best_estimator_rf = grid_search_rf.best_estimator_\n",
    "    \n",
    "    return best_estimator_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    #Preprocessing functions\n",
    "    hidden_data = hidden_data.drop(['clntnum'], axis = 1)\n",
    "    hidden_data = hidden_data.drop(['min_occ_date'], axis = 1)\n",
    "    hidden_data = hidden_data.drop(['cltdob_fix'], axis = 1)\n",
    "    thresh = 50\n",
    "    percentage = [col for col in hidden_data if hidden_data[col].isna().sum()/hidden_data.shape[0]*100>thresh]\n",
    "    hidden_data = hidden_data.drop(percentage, axis=1)\n",
    "    cate_data = [col for col in hidden_data.columns if hidden_data[col].dtype not in ['float64', 'int64']]\n",
    "    for col in cate_data:\n",
    "        mode_value = hidden_data[col].mode().iloc[0]  # Use iloc[0] to get the first mode in case of multiple modes\n",
    "        hidden_data[col] = hidden_data[col].fillna(mode_value)\n",
    "    decimal_columns = ['hh_20', 'pop_20', 'hh_size', 'n_months_last_bought_products', 'flg_latest_being_lapse', 'flg_latest_being_cancel', 'tot_inforce_pols',\n",
    "                    'ape_gi_42e115', 'ape_ltc_1280bf', 'ape_grp_6fc3e6', 'ape_grp_de05ae', 'ape_inv_dcd836', 'ape_grp_945b5a',\n",
    "                  'ape_grp_6a5788', 'ape_ltc_43b9d5', 'ape_grp_9cdedf', 'ape_lh_d0adeb', 'ape_grp_1581d7', 'ape_grp_22decf',\n",
    "                  'ape_lh_507c37', 'ape_lh_839f8a', 'ape_inv_e9f316', 'ape_gi_a10d1b', 'ape_gi_29d435', 'ape_grp_caa6ff', \n",
    "                  'ape_grp_fd3bfb', 'ape_lh_e22a6a', 'ape_grp_70e1dd', 'ape_grp_e04c3a', 'ape_grp_fe5fb8', 'ape_gi_856320', \n",
    "                   'ape_grp_94baec', 'ape_gi_058815', 'ape_grp_e91421', 'ape_lh_f852af', 'ape_lh_947b15', 'sumins_gi_42e115', \n",
    "                   'sumins_ltc_1280bf', 'sumins_grp_6fc3e6', 'sumins_grp_de05ae', 'sumins_inv_dcd836', 'sumins_grp_945b5a', 'sumins_grp_6a5788', \n",
    "                  'sumins_ltc_43b9d5', 'sumins_grp_9cdedf', 'sumins_lh_d0adeb', 'sumins_grp_1581d7', 'sumins_grp_22decf', 'sumins_lh_507c37', \n",
    "                  'sumins_inv_e9f316', 'sumins_gi_a10d1b', 'sumins_gi_29d435', 'sumins_grp_caa6ff', 'sumins_grp_fd3bfb', 'sumins_lh_e22a6a', \n",
    "                  'sumins_grp_70e1dd', 'sumins_grp_e04c3a', 'sumins_grp_fe5fb8', 'sumins_gi_856320', 'sumins_grp_94baec', 'sumins_gi_058815',\n",
    "                  'sumins_grp_e91421', 'sumins_lh_f852af', 'sumins_lh_947b15', 'sumins_32c74c', 'prempaid_gi_42e115', 'prempaid_ltc_1280bf', \n",
    "                  'prempaid_grp_6fc3e6', 'prempaid_grp_de05ae', 'prempaid_inv_dcd836', 'prempaid_grp_945b5a', 'prempaid_grp_6a5788', \n",
    "                  'prempaid_ltc_43b9d5', 'prempaid_grp_9cdedf', 'prempaid_lh_d0adeb', 'prempaid_grp_1581d7', 'prempaid_lh_507c37', 'prempaid_lh_839f8a', \n",
    "                  'prempaid_inv_e9f316', 'prempaid_gi_a10d1b', 'prempaid_gi_29d435', 'prempaid_grp_caa6ff', 'prempaid_grp_fd3bfb', 'prempaid_lh_e22a6a', \n",
    "                  'prempaid_grp_70e1dd', 'prempaid_grp_e04c3a', 'prempaid_grp_fe5fb8', 'prempaid_gi_856320', 'prempaid_grp_94baec', \n",
    "                  'prempaid_gi_058815', 'prempaid_grp_e91421', 'prempaid_lh_f852af', 'prempaid_lh_947b15', 'prempaid_32c74c', \n",
    "                  'ape_839f8a', 'ape_e22a6a', 'ape_d0adeb', 'ape_c4bda5', 'ape_ltc', 'ape_507c37', 'ape_gi', 'f_hold_839f8a', \n",
    "                  'f_hold_e22a6a', 'f_hold_d0adeb', 'f_hold_c4bda5', 'f_hold_ltc', 'f_hold_507c37', 'f_hold_gi', 'sumins_839f8a',\n",
    "                  'sumins_e22a6a', 'sumins_d0adeb', 'sumins_c4bda5', 'sumins_ltc', 'sumins_507c37', 'sumins_gi', 'prempaid_839f8a', 'prempaid_e22a6a', \n",
    "                  'prempaid_d0adeb', 'prempaid_c4bda5', 'prempaid_ltc', 'prempaid_507c37', 'prempaid_gi', 'n_months_last_bought_839f8a',\n",
    "                  'n_months_last_bought_e22a6a', 'n_months_last_bought_d0adeb', 'n_months_last_bought_c4bda5', 'n_months_last_bought_ltc', \n",
    "                   'n_months_last_bought_507c37', 'n_months_last_bought_gi', 'f_ever_bought_ltc_1280bf', 'f_ever_bought_grp_6fc3e6', 'f_ever_bought_grp_de05ae', \n",
    "                   'f_ever_bought_inv_dcd836', 'f_ever_bought_grp_945b5a', 'f_ever_bought_grp_6a5788', 'f_ever_bought_ltc_43b9d5', 'f_ever_bought_grp_9cdedf',\n",
    "                   'f_ever_bought_lh_d0adeb', 'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf', 'f_ever_bought_lh_507c37', 'f_ever_bought_lh_839f8a',\n",
    "                   'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff', 'f_ever_bought_grp_fd3bfb', 'f_ever_bought_lh_e22a6a', 'f_ever_bought_grp_70e1dd',\n",
    "                   'f_ever_bought_grp_e04c3a', 'f_ever_bought_grp_fe5fb8', 'f_ever_bought_grp_94baec', 'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af',\n",
    "                   'f_ever_bought_lh_947b15', 'f_ever_bought_32c74c', 'n_months_last_bought_ltc_1280bf', 'n_months_last_bought_grp_6fc3e6', 'n_months_last_bought_grp_de05ae', \n",
    "                  'n_months_last_bought_inv_dcd836', 'n_months_last_bought_grp_945b5a', 'n_months_last_bought_grp_6a5788', 'n_months_last_bought_ltc_43b9d5', \n",
    "                  'n_months_last_bought_grp_9cdedf', 'n_months_last_bought_lh_d0adeb', 'n_months_last_bought_grp_1581d7', 'n_months_last_bought_grp_22decf', \n",
    "                  'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_507c37', 'n_months_last_bought_lh_839f8a', 'n_months_last_bought_inv_e9f316', \n",
    "                  'n_months_last_bought_grp_caa6ff', 'n_months_last_bought_grp_fd3bfb', 'n_months_last_bought_lh_e22a6a', 'n_months_last_bought_grp_70e1dd', \n",
    "                  'n_months_last_bought_grp_e04c3a', 'n_months_last_bought_grp_fe5fb8', 'n_months_last_bought_grp_94baec', 'n_months_last_bought_grp_e91421', \n",
    "                  'n_months_last_bought_lh_f852af', 'n_months_last_bought_lh_947b15', 'n_months_last_bought_32c74c', 'f_elx', 'f_mindef_mha', 'f_retail', \n",
    "                   'ape_32c74c', 'prempaid_grp_22decf', 'f_ever_bought_839f8a', 'f_ever_bought_e22a6a', 'f_ever_bought_d0adeb', 'f_ever_bought_c4bda5',\n",
    "                   'f_ever_bought_ltc', 'f_ever_bought_507c37', 'f_ever_bought_gi']\n",
    "    for col in decimal_columns:\n",
    "        hidden_data[col] = list(map(float, hidden_data[col]))\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    processed_columns = ['race_desc', 'ctrycode_desc', 'clttype', 'stat_flag', 'cltsex_fix',\n",
    "                     'flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term', 'flg_is_rental_flat', \n",
    "                     'flg_has_health_claim', 'flg_has_life_claim', 'flg_gi_claim', 'flg_is_proposal', \n",
    "                    'flg_with_preauthorisation', 'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
    "                    'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email', 'is_housewife_retiree',\n",
    "                    'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy', 'hh_size_est', 'annual_income_est']\n",
    "    preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(sparse_output=False, drop='first'), processed_columns)],remainder='passthrough')\n",
    "    data_encoded = preprocessor.fit_transform(hidden_data)\n",
    "    hidden_data = pd.DataFrame(data_encoded, columns=preprocessor.get_feature_names_out())\n",
    "    num_data = [col for col in hidden_data.columns if hidden_data[col].dtype in ['float64', 'int64']]\n",
    "    for col in num_data:\n",
    "        median_value = hidden_data[col].median()\n",
    "        hidden_data[col] = hidden_data[col].fillna(median_value)\n",
    "    not_selected = ['cat__ctrycode_desc_Bosnia-Herzegovina', 'cat__ctrycode_desc_Brunei Darussalam', 'cat__ctrycode_desc_Canada', 'cat__ctrycode_desc_China', 'cat__ctrycode_desc_Denmark', 'cat__ctrycode_desc_Hong Kong', 'cat__ctrycode_desc_Indonesia', 'cat__ctrycode_desc_Ireland', 'cat__ctrycode_desc_Italy', 'cat__ctrycode_desc_Japan', 'cat__ctrycode_desc_Malaysia', 'cat__ctrycode_desc_Netherlands', 'cat__ctrycode_desc_New Zealand', 'cat__ctrycode_desc_Not Applicable', 'cat__ctrycode_desc_Philippines', 'cat__ctrycode_desc_Singapore', 'cat__ctrycode_desc_South Africa', 'cat__ctrycode_desc_Spain', 'cat__ctrycode_desc_Sweden', 'cat__ctrycode_desc_Taiwan (R.O.C)', 'cat__ctrycode_desc_Thailand', 'cat__ctrycode_desc_United Arab Emirates', 'cat__ctrycode_desc_United Kingdom', 'cat__ctrycode_desc_United States', 'cat__ctrycode_desc_Unknown Country Code', 'cat__stat_flag_MATURED', 'cat__flg_substandard_nan', 'cat__flg_is_borderline_standard_nan', 'cat__flg_is_revised_term_1.0', 'cat__flg_is_revised_term_nan', 'cat__flg_is_rental_flat_1.0', 'cat__flg_is_rental_flat_nan', 'cat__flg_has_health_claim_nan', 'cat__flg_has_life_claim_1.0', 'cat__flg_has_life_claim_nan', 'cat__flg_gi_claim_nan', 'cat__flg_is_proposal_nan', 'cat__flg_with_preauthorisation_1.0', 'cat__flg_with_preauthorisation_nan', 'cat__flg_is_returned_mail_1.0', 'cat__flg_is_returned_mail_nan', 'cat__is_consent_to_mail_nan', 'cat__is_consent_to_email_nan', 'cat__is_consent_to_call_nan', 'cat__is_consent_to_sms_nan', 'cat__is_valid_dm_nan', 'cat__is_valid_email_nan', 'cat__is_housewife_retiree_nan', 'cat__is_sg_pr_nan', 'cat__is_class_1_2_nan', 'cat__is_dependent_in_at_least_1_policy_nan', 'remainder__ape_gi_42e115', 'remainder__ape_ltc_1280bf', 'remainder__ape_grp_de05ae', 'remainder__ape_inv_dcd836', 'remainder__ape_grp_6a5788', 'remainder__ape_lh_d0adeb', 'remainder__ape_inv_e9f316', 'remainder__ape_gi_a10d1b', 'remainder__ape_gi_29d435', 'remainder__ape_grp_fd3bfb', 'remainder__ape_grp_e04c3a', 'remainder__ape_gi_856320', 'remainder__ape_gi_058815', 'remainder__ape_32c74c', 'remainder__sumins_gi_42e115', 'remainder__sumins_ltc_1280bf', 'remainder__sumins_grp_de05ae', 'remainder__sumins_inv_dcd836', 'remainder__sumins_grp_6a5788', 'remainder__sumins_lh_d0adeb', 'remainder__sumins_grp_22decf', 'remainder__sumins_inv_e9f316', 'remainder__sumins_gi_a10d1b', 'remainder__sumins_gi_29d435', 'remainder__sumins_grp_fd3bfb', 'remainder__sumins_lh_e22a6a', 'remainder__sumins_grp_e04c3a', 'remainder__sumins_grp_fe5fb8', 'remainder__sumins_gi_856320', 'remainder__sumins_grp_94baec', 'remainder__sumins_gi_058815', 'remainder__sumins_32c74c', 'remainder__prempaid_gi_42e115', 'remainder__prempaid_ltc_1280bf', 'remainder__prempaid_grp_de05ae', 'remainder__prempaid_inv_dcd836', 'remainder__prempaid_grp_6a5788', 'remainder__prempaid_lh_d0adeb', 'remainder__prempaid_grp_22decf', 'remainder__prempaid_inv_e9f316', 'remainder__prempaid_gi_a10d1b', 'remainder__prempaid_gi_29d435', 'remainder__prempaid_grp_fd3bfb', 'remainder__prempaid_grp_e04c3a', 'remainder__prempaid_gi_856320', 'remainder__prempaid_gi_058815', 'remainder__prempaid_32c74c', 'remainder__ape_d0adeb', 'remainder__ape_c4bda5', 'remainder__ape_gi', 'remainder__f_hold_d0adeb', 'remainder__f_hold_c4bda5', 'remainder__f_hold_gi', 'remainder__sumins_e22a6a', 'remainder__sumins_d0adeb', 'remainder__sumins_c4bda5', 'remainder__sumins_gi', 'remainder__prempaid_d0adeb', 'remainder__prempaid_c4bda5', 'remainder__prempaid_gi', 'remainder__f_ever_bought_d0adeb', 'remainder__f_ever_bought_c4bda5', 'remainder__n_months_last_bought_d0adeb', 'remainder__f_ever_bought_ltc_1280bf', 'remainder__f_ever_bought_grp_6fc3e6', 'remainder__f_ever_bought_grp_de05ae', 'remainder__f_ever_bought_inv_dcd836', 'remainder__f_ever_bought_grp_945b5a', 'remainder__f_ever_bought_grp_6a5788', 'remainder__f_ever_bought_grp_9cdedf', 'remainder__f_ever_bought_lh_d0adeb', 'remainder__f_ever_bought_grp_22decf', 'remainder__f_ever_bought_inv_e9f316', 'remainder__f_ever_bought_grp_fd3bfb', 'remainder__f_ever_bought_grp_e04c3a', 'remainder__f_ever_bought_grp_e91421', 'remainder__f_ever_bought_32c74c', 'remainder__n_months_last_bought_ltc_1280bf', 'remainder__n_months_last_bought_grp_de05ae', 'remainder__n_months_last_bought_inv_dcd836', 'remainder__n_months_last_bought_grp_6a5788', 'remainder__n_months_last_bought_lh_d0adeb', 'remainder__n_months_last_bought_inv_e9f316']\n",
    "    hidden_data = hidden_data.drop(not_selected,axis=1)\n",
    "    zero_std_columns = hidden_data.columns[hidden_data.std() == 0]\n",
    "    hidden_data = hidden_data.drop(columns=zero_std_columns)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    hidden_data = scaler.fit_transform(hidden_data)\n",
    "    hidden_data = pd.DataFrame(hidden_data)\n",
    "    y_pred_rf = train_model().predict(hidden_data)\n",
    "    result = y_pred_rf\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"]) \n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
